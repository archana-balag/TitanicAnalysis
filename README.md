# TitanicAnalysis
Dive into TitanicAnalysis, where we explore the Titanic dataset with machine learning models. Because who needs lifeboats when you have data science? Learn, laugh, and maybe even cry as we uncover insights and build models that (hopefully) won't hit an iceberg.


# Titanic Analysis: The Unsinkable Data Exploration
Welcome aboard the Titanic Analysis project! This repository contains a Google Colab notebook that dives into the infamous Titanic dataset. Who needs lifeboats when you have machine learning, right?

# Getting Started
# To embark on this journey:

# Clone this repository:
# bash
git clone https://github.com/archana-balag/TitanicAnalysis.git

# Navigate to the directory:
bash
cd TitanicAnalysis

# Open the notebook in Google Colab:
Go to Google Colab.
Click on File > Open notebook.
Select the GitHub tab.
Enter archana-balag/TitanicAnalysis in the search bar and open the titanic.ipynb notebook.

# Prerequisites
Before you board, make sure you have the following:

A Google account (because you’ll need Google Colab, and who doesn't have a Google account these days?).
Required Python packages (listed in the notebook itself, so you don’t have to worry about installing them manually).

# Usage
Open the titanic.ipynb file in Google Colab and run the cells to explore the analysis. Feel free to modify the notebook as you see fit, but remember, rearranging deck chairs on the Titanic didn't help much.

# Analysis Overview
Our notebook covers the following:

Exploratory Data Analysis (EDA): Where we pretend to understand the data.

# Data Preprocessing: 
Cleaning up the data mess because, yes, the Titanic was not a clean affair.

# Model Building: 
Because we all need a little decision-making support.

# Model Evaluation: 
Judging models like Simon Cowell, but nicer.

# Results

## Decision Tree
Accuracy: 0.9580

## Confusion Matrix: 
A tidy table showing 141 correct non-survivor predictions, 3 mistaken, 8 incorrect survivor predictions, and 110 accurate ones.

# K-Nearest Neighbors (KNN)
Accuracy: 0.9580

## Confusion Matrix: 
Almost a mirror image of the Decision Tree with similar numbers.

# Neural Network
Accuracy: 0.9656
## Confusion Matrix: 
Slightly better, because neural networks are fancy like that.

# Contributing
Think you can steer this ship better? Contributions are welcome. Submit a pull request, and if it doesn't sink the project, we'll merge it.

# License
This project is licensed under the MIT License. Basically, you can do whatever you want with it, but if you mess it up, it’s not our fault.

